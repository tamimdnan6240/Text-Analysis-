{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "## For Label Encoding \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "## Text \n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import os \n",
    "from nltk.corpus import wordnet\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77197abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d17a3",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002d5c7",
   "metadata": {},
   "source": [
    "Clean and transform the raw data into suitable data for further processing. \n",
    "\n",
    "Read this to grasp the text preprocessing ideas from this link: https://www.linkedin.com/pulse/text-preprocessing-natural-language-processing-nlp-germec-phd/ \n",
    "\n",
    "1. Tokenization: Break the text into smaller units\n",
    "2. Normalization: Converting texts into standard or common form like (0 to 1) \n",
    "3. Stemming: Reduce the words to their base form by removing the suffixes. So simplify the vocabulary. \n",
    "4. Lemmatization: This is the processing of reducing words to their root or base form by removing suffixes. For example, \"running\" can be stemmed to \"run\". reduce texts and reduce the vocabulary. \n",
    "5. Stopword removal: \n",
    "6. Punctuation removal: Remove commas, periods, question marks, or other punctuations from your text. \n",
    "7. Spelling correction: Correct spelling errors or typos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a local directory for NLTK data \n",
    "\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the necessary NLTK resources to the local directory \n",
    "nltk.download(\"stopwords\", download_dir=nltk_data_path)\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path)\n",
    "nltk.download(\"wordnert\", download_dir=nltk_data_path)\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6007d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set NLTK data path to the local path directory \n",
    "nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5019e0",
   "metadata": {},
   "source": [
    "## Convert to lowercase, strip and remove the punctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by performing the following steps:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Strips leading and trailing whitespaces.\n",
    "    3. Removes HTML tags.\n",
    "    4. Replaces punctuation with spaces.\n",
    "    5. Removes extra spaces.\n",
    "    6. Removes references (e.g., [1], [2]).\n",
    "    7. Removes non-word characters.\n",
    "    8. Removes digits.\n",
    "    9. Removes any extra spaces left after the above steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove HTML tags using regex\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # Replace punctuation with spaces\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # Remove references (e.g., [1], [2])\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    \n",
    "    # Remove non-word characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    \n",
    "    # Remove any extra spaces left after the above steps\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bec06f",
   "metadata": {},
   "source": [
    "## STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword(string):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input string.\n",
    "    \n",
    "    1. Define a set of English stopwords.\n",
    "    2. Split the input string into individual words.\n",
    "    3. Filter out words that are in the stopwords set.\n",
    "    4. Join the remaining words back into a single string.\n",
    "    \"\"\"\n",
    "    # Define a set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the input string into individual words and filter out stopwords\n",
    "    filtered_words = [word for word in string.split() if word not in stop_words]\n",
    "    \n",
    "    # Join the remaining words back into a single string\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ffa90",
   "metadata": {},
   "source": [
    "## Lemimatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06015021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to map NLTK position tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Maps NLTK POS tags to WordNet POS tags.\n",
    "    \n",
    "    Args:\n",
    "    tag (str): The POS tag from NLTK.\n",
    "    \n",
    "    Returns:\n",
    "    str: The corresponding WordNet POS tag.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        # If the tag starts with 'J', it corresponds to an adjective in WordNet\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        # If the tag starts with 'V', it corresponds to a verb in WordNet\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        # If the tag starts with 'N', it corresponds to a noun in WordNet\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        # If the tag starts with 'R', it corresponds to an adverb in WordNet\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # If the tag does not match any of the above, default to noun in WordNet\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0002a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the sentence and lemmatize\n",
    "def lemmatizer(string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string and lemmatizes each token based on its POS tag.\n",
    "    \n",
    "    Args:\n",
    "    string (str): The input text to be tokenized and lemmatized.\n",
    "    \n",
    "    Returns:\n",
    "    str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get position tags for each word/token in the input string\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string))\n",
    "    \n",
    "    # Lemmatize each word/token based on its POS tag\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
    "    \n",
    "    # Join the lemmatized words/tokens back into a single string\n",
    "    return \" \".join(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cf8ac",
   "metadata": {},
   "source": [
    "## Final Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f475e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    if isinstance(string, str) and string.strip():  # Check if the input is a non-empty string\n",
    "        return lemmatizer(stopword(preprocess(string)))\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if the input is not valid\n",
    "\n",
    "def apply_preprocessing(row):\n",
    "    try:\n",
    "        return finalpreprocess(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a189a90",
   "metadata": {},
   "source": [
    "## Load Your Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9901d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your new dataset\n",
    "df_test = pd.read_csv(\"C:\\\\Users\\\\tadnan\\\\OneDrive - Michigan Technological University\\\\Systematic Review\\\\Final_files_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure the column anmes are not used with Space \n",
    "df_test.rename(columns=lambda x:x.strip(), inplace=True)\n",
    "## Replace \"Yes \" with \"Yes\"\n",
    "df_test['Target'] = df_test['Target'].str.strip()\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd11f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Uncecessary Columns \n",
    "df_test.drop(['Unnamed: 0', 'Authors', 'Author full names', 'Author(s) ID', 'Title',\n",
    "       'Year', 'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start',\n",
    "       'Page end', 'Page count', 'Cited by', 'DOI', 'Link',\n",
    "       'Author Keywords', 'PubMed ID', 'Abbreviated Source Title',\n",
    "       'Document Type', 'Publication Stage', 'Open Access', 'Source', 'EID',\n",
    "       'Unnamed: 25', 'Reasons'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98aa7e3",
   "metadata": {},
   "source": [
    "## Check and remove the Nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62589805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983f114",
   "metadata": {},
   "source": [
    "## Label Encoding Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Target'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply label encodung to the \"Target\" column \n",
    "label_encoder = LabelEncoder()\n",
    "df_test['Target'] = label_encoder.fit_transform(df_test['Target'])\n",
    "print(type(df_test))\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Target'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b69956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Abstract' column\n",
    "df_test['Clean_Text_Abstract'] = df_test['Abstract'].apply(lambda x: apply_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the index of the \"Abstract column\"\n",
    "df_test.columns.get_loc('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0aa2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column order with 'clean_text' moved to the first position\n",
    "new_order = ['Clean_Text_Abstract'] + [col for col in df_test.columns if col != 'Clean_Text_Abstract']\n",
    "\n",
    "# Reindex the dataframe with the new column order\n",
    "df_test = df_test.reindex(columns=new_order)\n",
    "\n",
    "## Now drop your 'Abstract' \n",
    "df_test.drop(['Abstract'], axis=1, inplace=True)\n",
    "\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691db287",
   "metadata": {},
   "source": [
    "The TextDataset class handles the tokenization and preparation of the text data.\n",
    "\n",
    "The DataLoader is used to batch and shuffle the data for the validation phase.\n",
    "\n",
    "These components together facilitate efficient loading and processing of the data for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from PyTorch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Define the custom dataset class inheriting from PyTorch's Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - texts: List of text samples.\n",
    "        - labels: List of labels corresponding to the text samples.\n",
    "        - tokenizer: Tokenizer object used to convert text into tokens.\n",
    "        - max_len: Maximum length of token sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts  # Store the list of texts\n",
    "        self.labels = labels  # Store the list of labels\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer\n",
    "        self.max_len = max_len  # Store the maximum length of token sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \n",
    "        This method is required by the Dataset class and is used by DataLoader\n",
    "        to determine the size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.texts)  # Return the total number of texts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a sample from the dataset at the given index.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx: Index of the sample to retrieve.\n",
    "        \n",
    "        This method is required by the Dataset class and is used by DataLoader\n",
    "        to get a specific sample.\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]  # Get the text at the specified index\n",
    "        label = self.labels[idx]  # Get the label at the specified index\n",
    "\n",
    "        # Tokenize the text and encode it into token IDs, with padding and truncation\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,  # The text to encode\n",
    "            add_special_tokens=True,  # Add special tokens (like [CLS] and [SEP] for BERT)\n",
    "            max_length=self.max_len,  # Maximum length of the token sequence\n",
    "            return_token_type_ids=False,  # Do not return token type IDs\n",
    "            padding='max_length',  # Pad sequences to the maximum length\n",
    "            truncation=True,  # Truncate sequences that are longer than the maximum length\n",
    "            return_attention_mask=True,  # Return the attention mask to differentiate padding tokens\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Return a dictionary containing the input IDs, attention mask, and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),  # Flatten the input IDs tensor\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),  # Flatten the attention mask tensor\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert the label to a tensor of type long\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b1005",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_test['Clean_Text_Abstract']\n",
    "labels = df_test['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce18f9",
   "metadata": {},
   "source": [
    "## Introduce the Tokenizer and Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and device\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e2e06",
   "metadata": {},
   "source": [
    "The DataLoader is used to batch and shuffle the data for the validation phase.\n",
    "\n",
    "So, Facilitate efficient loading and processing of the data for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Prepare test datasets\n",
    "test_dataset = TextDataset(\n",
    "    texts=texts.values,\n",
    "    labels=labels.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66f61e",
   "metadata": {},
   "source": [
    "## Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_ABSRATRACT_CLASSIFIER import BERTClassifier \n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_classes = 2  # Binary classification\n",
    "bert_model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb313d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e66999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient calculation for testing\n",
    "with torch.no_grad():\n",
    "    # Loop over the batches of data in the test dataloader\n",
    "    for batch in test_dataloader:\n",
    "        # Move input data to the GPU (or CPU if device is not CUDA)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Perform a forward pass and get the model outputs\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the predictions by finding the index of the maximum logit\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        # Extend the list of predictions\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        # Extend the list of true labels\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fee4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the true labels and predicted labels\n",
    "df_predictions = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'ground_truth': all_labels,\n",
    "    'Predicted Labels': all_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame\n",
    "df_predictions.to_csv('Lebeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef876c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eae1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
