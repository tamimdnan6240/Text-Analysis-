{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b4dd33",
   "metadata": {},
   "source": [
    "Explanation of Each Code Line\n",
    "\n",
    "Imports: Import necessary libraries for deep learning and data handling.\n",
    "\n",
    "Initialization: Set up lists to store training and validation losses.\n",
    "\n",
    "Epoch Loop: Loop over the number of epochs specified by EPOCHS.\n",
    "\n",
    "Training Mode: Set the model to training mode.\n",
    "\n",
    "Initialize Metrics: Initialize variables to track loss and accuracy.\n",
    "\n",
    "Batch Loop: Loop over batches in the training dataloader.\n",
    "\n",
    "Data to Device: Move input data to the GPU or CPU.\n",
    "\n",
    "Zero Gradients: Clear previous gradients.\n",
    "\n",
    "Forward Pass: Get model outputs.\n",
    "\n",
    "Calculate Loss: Compute loss using the criterion.\n",
    "\n",
    "Accumulate Loss: Add the loss to the total training loss.\n",
    "\n",
    "Predictions: Get predictions by finding the index of the maximum logit.\n",
    "\n",
    "Count Correct Predictions: Increment the count of correct predictions.\n",
    "\n",
    "Total Predictions: Increment the total number of predictions.\n",
    "\n",
    "Backward Pass: Compute the gradients.\n",
    "\n",
    "Update Weights: Adjust model parameters.\n",
    "\n",
    "Average Loss: Compute the average training loss.\n",
    "\n",
    "Training Accuracy: Calculate training accuracy.\n",
    "\n",
    "Append Training Loss: Store the average training loss.\n",
    "\n",
    "Evaluation Mode: Set the model to evaluation mode.\n",
    "\n",
    "Validation Loop: Loop over batches in the validation dataloader.\n",
    "\n",
    "Data to Device: Move input data to the GPU or CPU.\n",
    "\n",
    "Forward Pass: Get model outputs.\n",
    "\n",
    "Calculate Loss: Compute loss using the criterion.\n",
    "\n",
    "Accumulate Loss: Add the loss to the total validation loss.\n",
    "\n",
    "Predictions: Get predictions by finding the index of the maximum logit.\n",
    "\n",
    "Count Correct Predictions: Increment the count of correct predictions.\n",
    "\n",
    "Total Predictions: Increment the total number of predictions.\n",
    "\n",
    "Average Loss: Compute the average validation loss.\n",
    "\n",
    "Validation Accuracy: Calculate validation accuracy.\n",
    "\n",
    "Append Validation Loss: Store the average validation loss.\n",
    "\n",
    "Print Metrics: Print training and validation metrics.\n",
    "\n",
    "Plot Losses: Plot the training and validation losses.\n",
    "\n",
    "Save Model: Save the model's state dictionary.\n",
    "\n",
    "Load Model: Load the saved state dictionary.\n",
    "\n",
    "Evaluation Mode: Set the model to evaluation mode.\n",
    "\n",
    "Initialize Predictions: Initialize lists to store predictions and labels.\n",
    "\n",
    "Testing Loop: Loop over batches in the test dataloader.\n",
    "\n",
    "Data to Device: Move input data to the GPU or CPU.\n",
    "\n",
    "Forward Pass: Get model outputs.\n",
    "\n",
    "Predictions: Get predictions by finding the index of the maximum logit.\n",
    "\n",
    "Store Predictions: Store predictions and true labels.\n",
    "\n",
    "Classification Report: Print a classification report of the results.\n",
    "\n",
    "Create DataFrame: Create a DataFrame to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn \n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import seaborn as sns \n",
    "\n",
    "## For bag of Words \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torchsummary import summary\n",
    "\n",
    "## For Label Encoding \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "## Text Preprocessing \n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "## TF-IDF (Term Frequency-Inverse Document Frequencies)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer \n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## apply a pipeline \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "## other pipelines \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\tadnan\\\\OneDrive - Michigan Technological University\\\\Systematic Review\\\\Final_files_.csv\"\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b073f3e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b87e75",
   "metadata": {},
   "source": [
    "### Column and Info Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a482e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6888f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53161a",
   "metadata": {},
   "source": [
    "## Remove additional spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure the column anmes are not used with Space \n",
    "data.rename(columns=lambda x:x.strip(), inplace=True)\n",
    "## Replace \"Yes \" with \"Yes\"\n",
    "data['Target'] = data['Target'].str.strip()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4727552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Uncecessary Columns \n",
    "data.drop(['Unnamed: 0', 'Authors', 'Author full names', 'Author(s) ID', 'Title',\n",
    "       'Year', 'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start',\n",
    "       'Page end', 'Page count', 'Cited by', 'DOI', 'Link',\n",
    "       'Author Keywords', 'PubMed ID', 'Abbreviated Source Title',\n",
    "       'Document Type', 'Publication Stage', 'Open Access', 'Source', 'EID',\n",
    "       'Unnamed: 25', 'Reasons'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec0ee9",
   "metadata": {},
   "source": [
    "## Missing Valyes Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abe362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef357f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca7fb0",
   "metadata": {},
   "source": [
    "### Select Random Samples \n",
    "data = data.sample(n=500)\n",
    "data.to_csv(\"Final_500_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d5830",
   "metadata": {},
   "source": [
    "## Label Encoding Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb48369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply label encodung to the \"Target\" column \n",
    "label_encoder = LabelEncoder()\n",
    "data['Target'] = label_encoder.fit_transform(data['Target'])\n",
    "print(type(data))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df = data[data['Target'] == 1] \n",
    "## type(df)\n",
    "## df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce2f8e",
   "metadata": {},
   "source": [
    "## Class Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['Target'].value_counts()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate value counts \n",
    "value_counts = data['Target'].value_counts()\n",
    "\n",
    "## Create a bar plt\n",
    "sns.barplot(x=value_counts.index, y =value_counts.values)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Target Value Counts\")\n",
    "plt.savefig('Target Value Counts.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c40f",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53480313",
   "metadata": {},
   "source": [
    "Clean and transform the raw data into suitable data for further processing. \n",
    "\n",
    "Read this to grasp the text preprocessing ideas from this link: https://www.linkedin.com/pulse/text-preprocessing-natural-language-processing-nlp-germec-phd/ \n",
    "\n",
    "1. Tokenization: Break the text into smaller units\n",
    "2. Normalization: Converting texts into standard or common form like (0 to 1) \n",
    "3. Stemming: Reduce the words to their base form by removing the suffixes. So simplify the vocabulary. \n",
    "4. Lemmatization: This is the processing of reducing words to their root or base form by removing suffixes. For example, \"running\" can be stemmed to \"run\". reduce texts and reduce the vocabulary. \n",
    "5. Stopword removal: \n",
    "6. Punctuation removal: Remove commas, periods, question marks, or other punctuations from your text. \n",
    "7. Spelling correction: Correct spelling errors or typos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a local directory for NLTK data \n",
    "import os \n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the necessary NLTK resources to the local directory \n",
    "nltk.download(\"stopwords\", download_dir=nltk_data_path)\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path)\n",
    "nltk.download(\"wordnert\", download_dir=nltk_data_path)\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set NLTK data path to the local path directory \n",
    "nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbf0ee",
   "metadata": {},
   "source": [
    "## Convert to lowercase, strip and remove the punctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by performing the following steps:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Strips leading and trailing whitespaces.\n",
    "    3. Removes HTML tags.\n",
    "    4. Replaces punctuation with spaces.\n",
    "    5. Removes extra spaces.\n",
    "    6. Removes references (e.g., [1], [2]).\n",
    "    7. Removes non-word characters.\n",
    "    8. Removes digits.\n",
    "    9. Removes any extra spaces left after the above steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove HTML tags using regex\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # Replace punctuation with spaces\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # Remove references (e.g., [1], [2])\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    \n",
    "    # Remove non-word characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    \n",
    "    # Remove any extra spaces left after the above steps\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76a371",
   "metadata": {},
   "source": [
    "## STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c69508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopword(string):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input string.\n",
    "    \n",
    "    1. Define a set of English stopwords.\n",
    "    2. Split the input string into individual words.\n",
    "    3. Filter out words that are in the stopwords set.\n",
    "    4. Join the remaining words back into a single string.\n",
    "    \"\"\"\n",
    "    # Define a set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the input string into individual words and filter out stopwords\n",
    "    filtered_words = [word for word in string.split() if word not in stop_words]\n",
    "    \n",
    "    # Join the remaining words back into a single string\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9efa9b",
   "metadata": {},
   "source": [
    "## Lemimatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ab408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb295ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# This is a helper function to map NLTK position tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Maps NLTK POS tags to WordNet POS tags.\n",
    "    \n",
    "    Args:\n",
    "    tag (str): The POS tag from NLTK.\n",
    "    \n",
    "    Returns:\n",
    "    str: The corresponding WordNet POS tag.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        # If the tag starts with 'J', it corresponds to an adjective in WordNet\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        # If the tag starts with 'V', it corresponds to a verb in WordNet\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        # If the tag starts with 'N', it corresponds to a noun in WordNet\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        # If the tag starts with 'R', it corresponds to an adverb in WordNet\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # If the tag does not match any of the above, default to noun in WordNet\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the sentence and lemmatize\n",
    "def lemmatizer(string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string and lemmatizes each token based on its POS tag.\n",
    "    \n",
    "    Args:\n",
    "    string (str): The input text to be tokenized and lemmatized.\n",
    "    \n",
    "    Returns:\n",
    "    str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get position tags for each word/token in the input string\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string))\n",
    "    \n",
    "    # Lemmatize each word/token based on its POS tag\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
    "    \n",
    "    # Join the lemmatized words/tokens back into a single string\n",
    "    return \" \".join(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403eb296",
   "metadata": {},
   "source": [
    "## Final Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    if isinstance(string, str) and string.strip():  # Check if the input is a non-empty string\n",
    "        return lemmatizer(stopword(preprocess(string)))\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if the input is not valid\n",
    "\n",
    "def apply_preprocessing(row):\n",
    "    try:\n",
    "        return finalpreprocess(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34cfc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Abstract' column\n",
    "data['Clean_Text_Abstract'] = data['Abstract'].apply(lambda x: apply_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3025f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the index of the \"Abstract column\"\n",
    "data.columns.get_loc('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a8c34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the new column order with 'clean_text' moved to the first position\n",
    "new_order = ['Clean_Text_Abstract'] + [col for col in data.columns if col != 'Clean_Text_Abstract']\n",
    "\n",
    "# Reindex the dataframe with the new column order\n",
    "data = data.reindex(columns=new_order)\n",
    "\n",
    "## Now drop your 'Abstract' \n",
    "data.drop(['Abstract'], axis=1, inplace=True)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1669d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbad10a",
   "metadata": {},
   "source": [
    "## Read this link: https://medium.com/@claude.feldges/text-classification-with-tf-idf-lstm-bert-a-quantitative-comparison-b8409b556cb3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28a2c8",
   "metadata": {},
   "source": [
    "### Source : https://www.sabrepc.com/blog/Deep-Learning-and-AI/text-classification-with-bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9625b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a6a48",
   "metadata": {},
   "source": [
    "## Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Clean_Text_Abstract'] \n",
    "Y = data['Target'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(df):\n",
    "    texts = df['Clean_Text_Abstract']\n",
    "    labels = df['Target']\n",
    "    return texts, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels  = load_file(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6684c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e92e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0f1a1",
   "metadata": {},
   "source": [
    "## Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0dc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and device\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from PyTorch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Define the custom dataset class inheriting from PyTorch's Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - texts: List of text samples.\n",
    "        - labels: List of labels corresponding to the text samples.\n",
    "        - tokenizer: Tokenizer object used to convert text into tokens.\n",
    "        - max_len: Maximum length of token sequences.\n",
    "        \"\"\"\n",
    "        self.texts = texts  # Store the list of texts\n",
    "        self.labels = labels  # Store the list of labels\n",
    "        self.tokenizer = tokenizer  # Store the tokenizer\n",
    "        self.max_len = max_len  # Store the maximum length of token sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \n",
    "        This method is required by the Dataset class and is used by DataLoader\n",
    "        to determine the size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.texts)  # Return the total number of texts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a sample from the dataset at the given index.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx: Index of the sample to retrieve.\n",
    "        \n",
    "        This method is required by the Dataset class and is used by DataLoader\n",
    "        to get a specific sample.\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]  # Get the text at the specified index\n",
    "        label = self.labels[idx]  # Get the label at the specified index\n",
    "\n",
    "        # Tokenize the text and encode it into token IDs, with padding and truncation\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,  # The text to encode\n",
    "            add_special_tokens=True,  # Add special tokens (like [CLS] and [SEP] for BERT)\n",
    "            max_length=self.max_len,  # Maximum length of the token sequence\n",
    "            return_token_type_ids=False,  # Do not return token type IDs\n",
    "            padding='max_length',  # Pad sequences to the maximum length\n",
    "            truncation=True,  # Truncate sequences that are longer than the maximum length\n",
    "            return_attention_mask=True,  # Return the attention mask to differentiate padding tokens\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Return a dictionary containing the input IDs, attention mask, and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),  # Flatten the input IDs tensor\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),  # Flatten the attention mask tensor\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert the label to a tensor of type long\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_texts.values,\n",
    "    labels=train_labels.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "Val_dataset = TextDataset(\n",
    "    texts=val_texts.values,\n",
    "    labels=val_labels.values,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ae806",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(Val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf722c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_ABSRATRACT_CLASSIFIER import BERTClassifier \n",
    "\n",
    "# Model parameters\n",
    "num_classes = 2  # Binary classification\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming model, criterion, optimizer, train_dataloader, val_dataloader, and test_dataloader are already defined\n",
    "\n",
    "# Lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 100 # Example, adjust accordingly\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "    # Initialize the number of correct predictions for this epoch\n",
    "    correct_train_predictions = 0\n",
    "    # Initialize the total number of predictions for this epoch\n",
    "    total_train_predictions = 0\n",
    "    \n",
    "    # Loop over the batches of data in the training dataloader\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Move input data to the GPU (or CPU if device is not CUDA)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Clear the gradients of all optimized tensors\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform a forward pass and get the model outputs\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Calculate the loss using the criterion\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Accumulate the loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Get the predictions by finding the index of the maximum logit\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        # Count the number of correct predictions\n",
    "        correct_train_predictions += torch.sum(preds == labels)\n",
    "        # Count the total number of predictions\n",
    "        total_train_predictions += labels.size(0)\n",
    "        \n",
    "        # Perform backpropagation to compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate the average loss over all batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Calculate the accuracy as the ratio of correct predictions to total predictions\n",
    "    train_accuracy = correct_train_predictions.double() / total_train_predictions\n",
    "    # Append the average training loss to the list\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize the total validation loss for this epoch\n",
    "    total_val_loss = 0\n",
    "    # Initialize the number of correct validation predictions for this epoch\n",
    "    correct_val_predictions = 0\n",
    "    # Initialize the total number of validation predictions for this epoch\n",
    "    total_val_predictions = 0\n",
    "    \n",
    "    # Disable gradient calculation for validation\n",
    "    with torch.no_grad():\n",
    "        # Loop over the batches of data in the validation dataloader\n",
    "        for batch in val_dataloader:\n",
    "            # Move input data to the GPU (or CPU if device is not CUDA)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Perform a forward pass and get the model outputs\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Calculate the loss using the criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Accumulate the loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Get the predictions by finding the index of the maximum logit\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            # Count the number of correct predictions\n",
    "            correct_val_predictions += torch.sum(preds == labels)\n",
    "            # Count the total number of predictions\n",
    "            total_val_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate the average validation loss over all batches\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    # Calculate the accuracy as the ratio of correct predictions to total predictions\n",
    "    val_accuracy = correct_val_predictions.double() / total_val_predictions\n",
    "    # Append the average validation loss to the list\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print the epoch number, average training loss, and training accuracy\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'Train Loss: {avg_train_loss} | Train Accuracy: {train_accuracy}')\n",
    "    # Print the average validation loss and validation accuracy\n",
    "    print(f'Validation Loss: {avg_val_loss} | Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_wts = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print('Early stopping!')\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        break\n",
    "        \n",
    "# Save the best model weights\n",
    "torch.save(best_model_wts, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ea7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Plot training losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "# Plot validation losses\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "# Label for the x-axis\n",
    "plt.xlabel('Epochs')\n",
    "# Label for the y-axis\n",
    "plt.ylabel('Loss')\n",
    "# Display legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaafb8e",
   "metadata": {},
   "source": [
    "## Save and Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4986899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eec3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
