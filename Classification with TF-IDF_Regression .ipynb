{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn \n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import seaborn as sns \n",
    "\n",
    "## For bag of Words \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torchsummary import summary\n",
    "\n",
    "## For Label Encoding \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "## Text Preprocessing \n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "## TF-IDF (Term Frequency-Inverse Document Frequencies)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer \n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## apply a pipeline \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "## other pipelines \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\tadnan\\\\OneDrive - Michigan Technological University\\\\Systematic Review\\\\Final_files_.csv\"\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b073f3e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b87e75",
   "metadata": {},
   "source": [
    "### Column and Info Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a482e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6888f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53161a",
   "metadata": {},
   "source": [
    "## Remove additional spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure the column anmes are not used with Space \n",
    "data.rename(columns=lambda x:x.strip(), inplace=True)\n",
    "## Replace \"Yes \" with \"Yes\"\n",
    "data['Target'] = data['Target'].str.strip()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4727552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Uncecessary Columns \n",
    "data.drop(['Unnamed: 0', 'Authors', 'Author full names', 'Author(s) ID', 'Title',\n",
    "       'Year', 'Source title', 'Volume', 'Issue', 'Art. No.', 'Page start',\n",
    "       'Page end', 'Page count', 'Cited by', 'DOI', 'Link',\n",
    "       'Author Keywords', 'PubMed ID', 'Abbreviated Source Title',\n",
    "       'Document Type', 'Publication Stage', 'Open Access', 'Source', 'EID',\n",
    "       'Unnamed: 25', 'Reasons'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec0ee9",
   "metadata": {},
   "source": [
    "## Missing Valyes Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abe362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef357f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select Random Samples \n",
    "data = data.sample(n=500)\n",
    "data.to_csv(\"Final_500_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d5830",
   "metadata": {},
   "source": [
    "## Label Encoding Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb48369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply label encodung to the \"Target\" column \n",
    "label_encoder = LabelEncoder()\n",
    "data['Target'] = label_encoder.fit_transform(data['Target'])\n",
    "print(type(data))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df = data[data['Target'] == 1] \n",
    "## type(df)\n",
    "## df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce2f8e",
   "metadata": {},
   "source": [
    "## Class Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['Target'].value_counts()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate value counts \n",
    "value_counts = data['Target'].value_counts()\n",
    "\n",
    "## Create a bar plt\n",
    "sns.barplot(x=value_counts.index, y =value_counts.values)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Target Value Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c40f",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53480313",
   "metadata": {},
   "source": [
    "Clean and transform the raw data into suitable data for further processing. \n",
    "\n",
    "Read this to grasp the text preprocessing ideas from this link: https://www.linkedin.com/pulse/text-preprocessing-natural-language-processing-nlp-germec-phd/ \n",
    "\n",
    "1. Tokenization: Break the text into smaller units\n",
    "2. Normalization: Converting texts into standard or common form like (0 to 1) \n",
    "3. Stemming: Reduce the words to their base form by removing the suffixes. So simplify the vocabulary. \n",
    "4. Lemmatization: This is the processing of reducing words to their root or base form by removing suffixes. For example, \"running\" can be stemmed to \"run\". reduce texts and reduce the vocabulary. \n",
    "5. Stopword removal: \n",
    "6. Punctuation removal: Remove commas, periods, question marks, or other punctuations from your text. \n",
    "7. Spelling correction: Correct spelling errors or typos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a local directory for NLTK data \n",
    "import os \n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the necessary NLTK resources to the local directory \n",
    "nltk.download(\"stopwords\", download_dir=nltk_data_path)\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path)\n",
    "nltk.download(\"wordnert\", download_dir=nltk_data_path)\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set NLTK data path to the local path directory \n",
    "nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbf0ee",
   "metadata": {},
   "source": [
    "## Convert to lowercase, strip and remove the punctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preprocessing functions\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76a371",
   "metadata": {},
   "source": [
    "## STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c69508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword(string):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in string.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9efa9b",
   "metadata": {},
   "source": [
    "## Lemimatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ab408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb295ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence and lemmatize\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string))  # Get position tags\n",
    "    a = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]  # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403eb296",
   "metadata": {},
   "source": [
    "## Final Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    if isinstance(string, str) and string.strip():  # Check if the input is a non-empty string\n",
    "        return lemmatizer(stopword(preprocess(string)))\n",
    "    else:\n",
    "        return \"\"  # Return an empty string if the input is not valid\n",
    "\n",
    "def apply_preprocessing(row):\n",
    "    try:\n",
    "        return finalpreprocess(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34cfc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Abstract' column\n",
    "data['Clean_Text_Abstract'] = data['Abstract'].apply(lambda x: apply_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3025f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the index of the \"Abstract column\"\n",
    "data.columns.get_loc('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a8c34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the new column order with 'clean_text' moved to the first position\n",
    "new_order = ['Clean_Text_Abstract'] + [col for col in data.columns if col != 'Clean_Text_Abstract']\n",
    "\n",
    "# Reindex the dataframe with the new column order\n",
    "data = data.reindex(columns=new_order)\n",
    "\n",
    "## Now drop your 'Abstract' \n",
    "data.drop(['Abstract'], axis=1, inplace=True)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1669d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89147e9d",
   "metadata": {},
   "source": [
    "## X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111cf43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=data['Clean_Text_Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data['Target']\n",
    "Y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91bb94c",
   "metadata": {},
   "source": [
    "## Word Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c483d4e",
   "metadata": {},
   "source": [
    "It’s difficult to work with text data while building Machine learning models since these models need well-defined numerical data. The process to convert text data into numerical data/vector, is called vectorization or in the NLP world, word embedding. Bag-of-Words(BoW) and Word Embedding (with Word2Vec) are two well-known methods for converting text data to numerical data.\n",
    "\n",
    "There are a few versions of Bag of Words, corresponding to different words scoring methods. We use the Sklearn library to calculate the BoW numerical values using these approaches: \n",
    "\n",
    "Count vectors: It builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. \n",
    "\n",
    "Term Frequency-Inverse Document Frequencies (tf-Idf): Count vectors might not be the best representation for converting text data to numerical data. So, instead of simple counting, we can also use an advanced variant of the Bag-of-Words that uses the term frequency–inverse document frequency (or Tf-Idf). Basically, the value of a word increases proportionally to count in the document, but it is inversely proportional to the frequency of the word in the corpus. \n",
    "\n",
    "Word2Vec: One of the major drawbacks of using Bag-of-words techniques is that it can’t capture the meaning or relation of the words from vectors. Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network which is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b62b4",
   "metadata": {},
   "source": [
    "use TF-IDF (Term-Frequency-Inverse Document Frequencies): \n",
    "Basic Steps: \n",
    "1. Instantiate Vectorization \n",
    "2. Fit and transform the text data \n",
    "3. Convert the TF-IDF to an array \n",
    "4. Get the feature names (words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d314e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the TfidfVectorizer with maximum nr words and ngrams (1: single words, 2: two words in a row)\n",
    "vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_vector = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert the TF-IDF matrix to an array\n",
    "tfidf_array = X_vector.toarray()\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTF-IDF Array:\")\n",
    "print(tfidf_array)\n",
    "print(\"\\nTotal Number of Features:\", len(feature_names))\n",
    "print(\"\\nFeature Names:\")\n",
    "for feature in feature_names:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80da908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563e52f",
   "metadata": {},
   "source": [
    "## Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7afc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vector, Y, test_size=0.30, random_state=42, stratify=data['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ef2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0412448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in training data\n",
    "print(\"Class distribution in y_train:\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()/Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9779b",
   "metadata": {},
   "source": [
    "## Now you can see the representation of the train and test datasets by Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e00a72",
   "metadata": {},
   "source": [
    "## Apply Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023294a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_w2v = LogisticRegression(solver='liblinear', C=10, penalty='l2')\n",
    "lr_w2v.fit(X_train, y_train)\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test)\n",
    "y_prob = lr_w2v.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "print(\"Logistic Regression Model (W2v)\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_predict))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b68dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
